---
description: Create PRD (ML-Dev): A structured template and methodology for generating Product Requirements Documents to implement or debug training/evaluation/data-loading features so the pipeline matches our model intent.
globs:
alwaysApply: false
---
# Rule: Generating a Development PRD for Model Training & Evaluation (ML-Dev PRD)

## Goal
Guide an AI assistant in creating a detailed PRD (Markdown) for **internal development tasks** related to model training, evaluation, data loading, and debugging. The PRD should be clear, actionable, and suitable for a junior developer to implement so that the pipeline behaves consistently with our modeling intent.

## Prerequisite: Codebase Familiarization
Before interacting with the developer/researcher, briefly explore:
1. Existing architecture, modules, and conventions in:
   - `src/models/`, `src/datasets/`, `src/trainer/`, `configs/`, `scripts/train|evaluation|inference/`
2. Related or similar components that may be affected (e.g., loss functions, metrics, augmentation).
3. Technical constraints/dependencies (framework versions, GPU/CPU availability, runtime/memory limits).
4. App/repo-specific rules or standards (see `.cursor/rules/*` if present).

## Process
1. **Codebase Familiarization** (above).
2. **Ask Clarifying Questions** focused on the intended training/eval behavior.
3. **Generate PRD** with the structure below.
4. **Save PRD** as `<repo_root>/tasks/YYYY-MM-DD-[task-name].md`.

## Clarifying Questions (Examples)
- **Problem/Goal:** What training/eval behavior or bug are we addressing? What outcome proves it’s fixed/implemented?
- **Target Metric/Signal:** What primary metric(s) or checks matter here (e.g., loss curve shape, mAP@0.5:0.95, Dice, latency)?
- **Data Requirements:** Which dataset(s) and **split(s)/version(s)** are in scope? Any preprocessing/augmentation expectations?
- **Scope/Boundaries:** What is explicitly **out of scope** for this task (e.g., no new dataset acquisition, no production deployment)?
- **Compute/Runtime:** Available hardware, time/memory constraints, expected training time per epoch, batch size/AMP preferences.
- **Config/Logging:** What configs, CLI flags, logs, and plots are expected? Any experiment tracking (e.g., MLflow/W&B) to use?
- **Edge Cases/Failure Modes:** Any current symptoms (NaNs, OOM, non-determinism), or corner cases to exercise?

## PRD Structure

1. **Introduction/Overview**  
   Briefly describe the internal dev task (feature/bugfix) and the training/eval behavior it should enable or correct. State the goal.

2. **Goals**  
   List specific, measurable objectives, e.g.:
   - G1: Reproduce baseline run X and achieve ≥ Y on metric Z.
   - G2: Eliminate NaN spikes during warmup by implementing gradient clipping at N.
   - G3: End-to-end run completes within T hours on 1×GPU without OOM.

3. **User Stories (for devs/researchers)**  
   - As a **developer**, I can launch training with a single config and see key logs (loss, LR, grad-norm, memory).  
   - As a **researcher**, I can evaluate on the validation split and export a concise metrics table/plot.  
   - As a **maintainer**, I can reproduce this behavior on another machine using the same config and seed.

4. **Functional Requirements**  
   Numbered, concrete requirements, e.g.:
   1. The system **must** load dataset **D(vX)** with transforms **T** and produce batches of shape **S**.
   2. Training **must** log loss, LR, grad-norm, and memory usage every **K** steps.
   3. Evaluation **must** compute metrics **M** (with thresholds) and write results to `outputs/[run-id]/metrics.json`.
   4. Provide a **tiny-subset overfit** script/flag to sanity-check learning behavior.
   5. Ensure runs are **reproducible** with a fixed seed; persist used configs and git commit hash.
   6. (If debugging) Provide a **minimal repro script** and document the fix with before/after evidence.

5. **Non-Goals (Out of Scope)**  
   Clearly state what this task will **not** do (e.g., no new model family, no multi-node training, no UI).

6. **Design Considerations (Optional)**  
   Config keys, CLI interface, logging schema, directory layout for artifacts, plot expectations.

7. **Technical Considerations (Optional)**  
   Framework/tooling versions, GPU type, AMP level, batch size vs. gradient accumulation, known memory constraints.

8. **Success Metrics (Definition of Done)**  
   - Metric/behavior targets achieved (e.g., “≥ 0.49 mAP on val, ±0.02 across 3 seeds”).  
   - No NaNs/inf; no OOM under specified batch/AMP.  
   - Reproducible: same config+seed yields similar results on a second run.  
   - Artifacts saved (checkpoints, logs, plots, metrics file).

9. **Open Questions**  
   Any unresolved decisions or follow-ups (e.g., final augmentation policy, seed count, exact metric threshold).

## Target Audience
Assume the primary reader is a **junior developer** working on the ML codebase. Avoid jargon where possible and be explicit about expected configs, logs, and artifacts.

## Output
- **Format:** Markdown (`.md`)  
- **Location:** `<repo_root>/tasks/`  
- **Filename:** `YYYY-MM-DD-[task-name].md`

## Final Instructions
1. **Do NOT** implement; only generate the PRD.  
2. Ask the clarifying questions first.  
3. Incorporate the answers and codebase findings into the final PRD.
