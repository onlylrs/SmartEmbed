#!/usr/bin/env python3
"""
Main training script for Jina Embeddings V4 fine-tuning

"""

import sys
import os
import yaml
import argparse
import logging
from pathlib import Path
from typing import Optional

# Add project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import torch
from transformers import TrainingArguments, set_seed

# Import our modules
from jina.models.modeling_jina_embeddings_v4 import JinaEmbeddingsV4Model, JinaEmbeddingsV4Processor
from jina.models.configuration_jina_embeddings_v4 import JinaEmbeddingsV4Config
from jina.training.jina_trainer import JinaEmbeddingTrainer, setup_model_for_training
from jina.training.training_config import JinaTrainingConfig

# Import Liam's data loading solution
from src.datasets.multimodal_dataset import get_training_dataloader

# Setup logging
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Train Jina Embeddings V4 model")
    
    # If no arguments provided, use config file mode
    parser.add_argument("--config_mode", action="store_true", 
                       help="Use project_config.yaml for all settings (default behavior)")
    
    # Data arguments (optional, override config)
    parser.add_argument("--train_data", type=str, help="Path to training data")
    parser.add_argument("--eval_data", type=str, help="Path to evaluation data")
    
    # Training arguments (optional, override config)
    parser.add_argument("--epochs", type=int, help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, help="Training batch size")
    parser.add_argument("--learning_rate", type=float, help="Learning rate")
    
    return parser.parse_args()


def load_config():
    """Load configuration from project_config.yaml"""
    config_path = project_root / "project_config.yaml"
    if not config_path.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    return config


def create_training_config(project_config, args):
    """
    é…ç½®è½¬æ¢æ¡¥æ¥å‡½æ•°
    
    åŠŸèƒ½ï¼šå°†ç”¨æˆ·å‹å¥½çš„ project_config.yaml è½¬æ¢ä¸ºä»£ç éœ€è¦çš„ JinaTrainingConfig å¯¹è±¡
    
    è½¬æ¢æ˜ å°„ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ project_config.yaml (ç”¨æˆ·ç¼–è¾‘)  â†’  JinaTrainingConfig (ä»£ç ä½¿ç”¨)    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ base_model_path                 â†’  model_name_or_path               â”‚
    â”‚ training.epochs                 â†’  num_train_epochs                 â”‚
    â”‚ training.batch_size             â†’  per_device_train_batch_size      â”‚
    â”‚ training.learning_rate          â†’  learning_rate                    â”‚
    â”‚ system.max_seq_length           â†’  max_seq_length                   â”‚
    â”‚ lora.*                          â†’  lora_* (ç›´æ¥æ˜ å°„)                â”‚
    â”‚ (é»˜è®¤å€¼)                        â†’  temperature, margin, ç­‰æŸå¤±å‡½æ•°å‚æ•° â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªå‡½æ•°ï¼š
    1. JinaEmbeddingTrainer å¿…é¡»è¦ JinaTrainingConfig å¯¹è±¡æ¥åˆå§‹åŒ–æŸå¤±å‡½æ•°
    2. ç”¨æˆ·ä¸åº”è¯¥ç¼–è¾‘åŒ…å«100+å‚æ•°çš„å¤æ‚é…ç½®æ–‡ä»¶
    3. æŠ€æœ¯å‚æ•°(å¦‚ temperature)ä½¿ç”¨ç»è¿‡éªŒè¯çš„é»˜è®¤å€¼
    
    Args:
        project_config: ä» project_config.yaml åŠ è½½çš„å­—å…¸
        args: å‘½ä»¤è¡Œå‚æ•°ï¼Œå¯ä»¥è¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®
        
    Returns:
        JinaTrainingConfig: åŒ…å«æ‰€æœ‰å¿…éœ€å‚æ•°çš„å®Œæ•´é…ç½®å¯¹è±¡
    """
    
    # Override with command line arguments if provided
    epochs = args.epochs or project_config['training']['epochs']
    batch_size = args.batch_size or project_config['training']['batch_size']
    learning_rate = args.learning_rate or project_config['training']['learning_rate']
    
    # Create JinaTrainingConfig with project_config values
    training_config = JinaTrainingConfig(
        # Model settings (from project_config)
        model_name_or_path=project_config['base_model_path'],
        trust_remote_code=True,
        
        # Training hyperparameters (from project_config + args override)
        output_dir=str(project_root / project_config['training']['output_dir'] / 'finetuned'),
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        learning_rate=learning_rate,
        
        # System settings (from project_config)
        max_seq_length=project_config['system']['max_seq_length'],
        
        # LoRA settings (from project_config)
        use_lora=project_config['training']['use_lora'],
        lora_r=project_config['lora']['r'],
        lora_alpha=project_config['lora']['alpha'],
        lora_dropout=project_config['lora']['dropout'],
        
        # Loss function settings (ä½¿ç”¨ç»è¿‡éªŒè¯çš„é»˜è®¤å€¼)
        # è¿™äº›å‚æ•°å½±å“å¯¹æ¯”å­¦ä¹ çš„æ•ˆæœï¼Œä½¿ç”¨ Jina æ¨èçš„å€¼
        temperature=0.02,                    # å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°
        margin=0.0,                         # æŸå¤±å‡½æ•°è¾¹ç•Œ
        matryoshka_dims=[128, 256, 512, 1024],  # å¤šç»´åº¦åµŒå…¥æ”¯æŒ
        use_matryoshka=False,               # æš‚æ—¶ç¦ç”¨ï¼Œä¸“æ³¨åŸºç¡€è®­ç»ƒ
    )
    
    return training_config


def main():
    """Main training function"""
    args = parse_args()
    
    # Load configuration
    config = load_config()
    
    # Create unified training configuration
    training_config = create_training_config(config, args)
    
    print("=== Jina Embeddings V4 Training ===")
    print(f"Project root: {project_root}")
    print(f"Base model path: {training_config.model_name_or_path}")
    
    # Validate base model path
    base_model_path = Path(training_config.model_name_or_path)
    if not base_model_path.exists():
        print(f"âŒ Base model path does not exist: {base_model_path}")
        sys.exit(1)
    
    # Set up training parameters from unified config
    train_data_path = args.train_data or str(project_root / config['data']['processed_dir'] / 'train.jsonl')
    eval_data_path = args.eval_data if args.eval_data else None
    
    output_dir = Path(training_config.output_dir)
    
    print(f"ğŸ“Š Training data: {train_data_path}")
    print(f"ğŸ“ˆ Training config: {training_config.num_train_epochs} epochs, batch_size={training_config.per_device_train_batch_size}, lr={training_config.learning_rate}")
    print(f"ğŸ’¾ Output directory: {output_dir}")
    
    # Check training data exists
    if not Path(train_data_path).exists():
        print(f"âŒ Training data not found: {train_data_path}")
        print("ğŸ’¡ Please create training data in the specified path")
        sys.exit(1)
    
    # Set random seed
    set_seed(42)
    
    try:
        # Load data using Liam's solution
        # ---- NEW: build real dataloader via src.datasets.multimodal_dataset ---- #
        from src.datasets.multimodal_dataset import get_training_dataloader

        train_dataloader = get_training_dataloader(data_config)
        train_dataset = train_dataloader.dataset
        
        # Load model and processor
        logger.info("Loading model and processor...")
        processor = JinaEmbeddingsV4Processor.from_pretrained(str(base_model_path))
        model = JinaEmbeddingsV4Model.from_pretrained(str(base_model_path))
        
        # Setup model for training
        if training_config.use_lora:
            model = setup_model_for_training(
                model,
                use_lora=True,
                lora_r=training_config.lora_r,
                lora_alpha=training_config.lora_alpha,
                lora_dropout=training_config.lora_dropout
            )
        
        # Create dataset (will be replaced with Liam's dataloader)
        # from jina.data.jina_dataset import JinaContrastiveDataset
        
        # train_dataset = JinaContrastiveDataset(
        #     examples=train_examples,
        #     processor=processor,
        #     max_length=training_config.max_seq_length,
        # )
        
        eval_dataset = None
        # if eval_examples:
        #     eval_dataset = JinaContrastiveDataset(
        #         examples=eval_examples,
        #         processor=processor,
        #         max_length=training_config.max_seq_length,
        #     )
        
        # Set up training arguments
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            num_train_epochs=int(training_config.num_train_epochs),
            per_device_train_batch_size=int(training_config.per_device_train_batch_size),
            per_device_eval_batch_size=int(training_config.per_device_train_batch_size),
            learning_rate=float(training_config.learning_rate),
            warmup_steps=100,
            logging_steps=10,
            save_steps=500,
            eval_steps=500 if eval_dataset else None,
            eval_strategy="steps" if eval_dataset else "no",  # Fixed back to correct parameter name
            save_strategy="steps",
            load_best_model_at_end=True if eval_dataset else False,
            metric_for_best_model="eval_loss" if eval_dataset else None,
            report_to="none",
            bf16=bool(config['system']['bf16']),  # Ensure boolean type
            dataloader_pin_memory=False,
        )
        
        # Initialize trainer with both TrainingArguments and JinaTrainingConfig
        trainer = JinaEmbeddingTrainer(
            model=model,
            training_config=training_config,  # ğŸ”¥ This was missing!
            training_args=training_args,
            tokenizer=processor,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
        )
        
        # Start training
        logger.info("Starting training...")
        trainer.train()
        
        # Save final model
        logger.info(f"Saving final model to {output_dir}")
        trainer.save_model()
        processor.save_pretrained(str(output_dir))
        
        print("ğŸ‰ Training completed successfully!")
        print(f"ğŸ“ Model saved to: {output_dir}")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        print(f"âŒ Training failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
